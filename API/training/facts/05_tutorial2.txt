* * *

title: "Tutorial 2: Scanning" sidebar_label: "Scanning"
hide_table_of_contents: true

* * *

<head> <html data-theme="dark" />

<meta name="description" content="A $1,000,000+ machine learning and computer
vision competition" />

<meta property="og:type" content="website" /> <meta property="og:url"
content="https://scrollprize.org" /> <meta property="og:title"
content="Vesuvius Challenge" /> <meta property="og:description" content="A
$1,000,000+ machine learning and computer vision competition" /> <meta
property="og:image" content="https://scrollprize.org/img/social/opengraph.jpg"
/>

<meta property="twitter:card" content="summary_large_image" /> <meta
property="twitter:url" content="https://scrollprize.org" /> <meta
property="twitter:title" content="Vesuvius Challenge" /> <meta
property="twitter:description" content="A $1,000,000+ machine learning and
computer vision competition" /> <meta property="twitter:image"
content="https://scrollprize.org/img/social/opengraph.jpg" /> </head>

import { TutorialsTop } from '@site/src/components/TutorialsTop'; import Tabs
from '@theme/Tabs'; import TabItem from '@theme/TabItem';

<TutorialsTop highlightId={2} />

### Campfire scroll

To get a basic understanding of the kind of data we’re working with, it’s
useful to look at it directly. Our goal will be to look at a scroll in two
ways: 1. a “word soup” of floating letters 2. a mesh representation:

<div className="flex flex-wrap items-start"> <figure className="sm:w-[48.4%]">
<img src="/img/tutorials/en-gedi-word-soup.gif"/> <figcaption
className="mt-0">Word soup in a simulated scroll <a
href="https://www.youtube.com/watch?v=tL7rhIFNtQg">(source)</a></figcaption>
</figure> <figure className="sm:w-[50%]"> <video autoPlay playsInline loop
muted className="w-[100%]" poster="/img/tutorials/meshlab-small2.jpg"> <source
src="/img/tutorials/meshlab-small2.webm" type="video/webm"/> <source
src="/img/tutorials/meshlab-small2.mp4" type="video/mp4"/> </video>
<figcaption className="mt-0">Mesh representation of the “campfire
scroll”</figcaption> </figure> </div>

We’ll use Stephen Parsons’s “campfire scroll” proxy (available on the <a
href="/data">Data page</a>). He made this scroll by writing on papyrus with
both carbon ink (low contrast with papyrus in CT scans) and iron gall ink
(high contrast with papyrus in CT scans). We won’t give away all the reference
pictures, but here is the last page of the scroll:

<figure className="max-w-[250px]">

<figcaption>Last page of the campfire scroll, with nonsense words</figcaption>
</figure>

Stephen then _carbonized_ the scroll in process that is similar to that of
making charcoal: he rolled up the scroll, sealed it in a coffee can, and
heated the can on a campfire. In a hot, oxygen-poor environment like the
coffee can, the papyrus turns to carbon rather than catching on fire. He then
put the resulting carbonized scroll in a benchtop CT scanner (Bruker SkyScan
1173).

<figure className=""> <div className="flex flex-wrap"> <img
className="max-h-[250px]" src="/img/tutorials/campfire-rolled2.jpg"/> <img
className="max-h-[250px]" src="/img/tutorials/campfire-rolled.jpg"/> <img
className="max-h-[250px]" src="/img/tutorials/campfire-camp.jpg"/> <img
className="max-h-[250px]" src="/img/tutorials/campfire-container.jpg"/> <img
className="max-h-[250px]" src="/img/tutorials/campfire-result.jpg"/> <img
className="max-h-[250px]" src="/img/tutorials/campfire-scanning.jpg"/> <img
className="max-h-[250px]" src="/img/tutorials/campfire-scanning2.jpg"/> </div>
<figcaption className="mt-1">Making of the campfire scroll</figcaption>
</figure>

### Fiji

We’ll use the “Fiji” open source program, which is a versatile tool for all
sorts of image-based operations. It is a distribution of ImageJ, but the main
version of ImageJ doesn’t include all the features we need. (In fact, “Fiji”
is a recursive acronym for “Fiji Is Just ImageJ”.)

:::info OS-specific instructions

<Tabs groupId="operating-systems"> <TabItem value="win" label="Windows">

  * No known issues.

</TabItem> <TabItem value="mac" label="macOS">

  * Even if you’re using an M1/M2 processor, please use the x86_64 version.

  * If you get a popup saying that the app can’t be opened, be sure _right click > Open_ to circumvent the blocking.

<figure className="max-w-[400px]">

</figure>

  * Alternatively, you can use the “Open Anyway” trick: go to _“System Preferences > Security & Privacy > General > Open Anyway”_.

<figure className="max-w-[400px]">

</figure>

</TabItem> <TabItem value="linux" label="Linux">

  * We recommend downloading the `.app` portable application from their website, instead of using a package manager. It does require you to have the JVM installed.

</TabItem>

</Tabs>

:::

<div>To follow along, download the tutorial data (campfire.zip) from the <a
href="/data">Data page</a>. In here there are three folders:</div>

  * `raw`: the raw X-ray photos of the scroll.
  * `rec`: the reconstructed 3D image volume (“rec” = “reconstructed”).
  * `logs`: log files during scanning and reconstruction.

Let’s start with `raw`. These are the raw X-ray photos of the scroll, taken in
0.15 degree steps for a full 360 degree rotation, so there are 2400 photos in
total (you can see these details in `logs/carbonized01-45kV_.log` and
`logs/carbonized01-45kV__TS.csv`).

However, to keep this tutorial dataset small, we have resized these photos to
1/4th their size and kept only every 4th photo. This effectively reduces the
voxel size of these scans from the original 26µm to 104µm. We also reduced the
bit depth from 16 bits to 8 bits.

We don’t typically use these photos directly, but they are useful to
understand how CT scanning works.

<figure className="max-w-[556px]">

</figure>

  1. In Fiji navigate to _“File > Import > Image Sequence”_. Then _“Browse”_ to select `campfire/raw`.
  2. The _“Count”_ should show exactly 600.<br/>

**If the count is not exactly 600** : double-check that the unzipping of the
dataset resulted in multiple directories. Sometimes zip programs put all files
inside one big directory. 3\. Click _“OK”_ to load the image stack.

:::tip

If Fiji crashes, or you run into other issues, try to increase the memory
limit: _“Edit > Options > Memory and Threads”._

:::

Note that when you are loading a dataset that hasn’t been reduced in size
beforehand, you can do that here. For example, we could have set _“Scale”_ to
_“25%”_ and _“Step”_ to _“4”_ , to get a similar result.

In the resulting scan you can vaguely see some features, perhaps even some
ink, but it's hard to see what is actually in there:

<figure> <video autoPlay playsInline loop muted className="max-w-[100%]
rounded-xl" poster="/img/tutorials/imagej-raw-xrays2.jpg"> <source
src="/img/tutorials/imagej-raw-xrays2.webm" type="video/webm"/> <source
src="/img/tutorials/imagej-raw-xrays2.mp4" type="video/mp4"/> </video>
</figure>

### Volumes

However, this is not yet the data we use for virtual unwrapping or machine
learning. For that, these x-ray projection images are combined using a process
called tomographic reconstruction. This is typically done by software that
comes with the CT scanner, so we already have done this, in our case using the
Bruker SkyScan NRecon tool.

The result of reconstruction is a 3D image, or volume, which is made up of 3D
pixel cubes called voxels. The _voxel size_ of the volume tells us the
physical size of the cube, and the value stored in the voxel is an estimate of
that location's radiodensity. Like the x-ray images, the volume is stored as a
.tif image stack, but this time each image represents a slice along the “z”
axis of the volume.

<figure className="max-w-[556px]">

</figure>

  * First close the existing window with the image stack.
  * Then open our reconstructed slices `campfire/rec` in the same way into Fiji ( _“File > Import > Image Sequence”_).
  * The count should be **477** now.

The result should look like this:

<figure> <video autoPlay playsInline loop muted className="w-[100%]
max-w-[400px] rounded-xl" poster="/img/tutorials/imagej-slices2.jpg"> <source
src="/img/tutorials/imagej-slices2.webm" type="video/webm"/> <source
src="/img/tutorials/imagej-slices2.mp4" type="video/mp4"/> </video> </figure>

In these images, radiodense materials have bright pixel values while more
radiolucent materials have darker pixel values. As a result, you can see the
iron gall ink light up quite clearly in these slices as bright lines which
follow the wraps of the scroll. However, it’s impossible to make out any
letters from this "top-down" view of the volume.

  * Go to _“Plugins > Volume Viewer”_.
    * It can take a while to load — there is a status bar with a loading bar on the main Fiji window.
    * In the top bar select _“Mode > Volume (4)”_
    * And _“Interpolation: Nearest Neighbor (0)”_
    * Check all 3 checkboxes at the bottom ( _“Axes”_ , _“Clipping”_ , and _“Slice positions”_ ).
  * Click the _“xy”_ button at the bottom so you get a top down view
    * Then click and drag up on the visualization to rotate it down.

<figure> <video autoPlay playsInline loop muted className="w-[100%]
max-w-[700px] rounded-xl" poster="/img/tutorials/drag-up2.jpg"> <source
src="/img/tutorials/drag-up2.webm" type="video/webm"/> <source
src="/img/tutorials/drag-up2.mp4" type="video/mp4"/> </video> </figure>

  * Next, draw the alpha graph to be roughly a diagonal line.
    * The x-axis represents the brightness of the color (left = dark, right = bright).
    * The y-axis represents the opacity that we’ll draw with.
    * A diagonal line means low opacity (= high translucency) for dark colors, which makes the parts without ink transparent, so we can see the ink shine through.
    * You have to do the drawing in one go, because Fiji can hang for a while when it decides to rerender the image.

<figure> <video autoPlay playsInline loop muted className="w-[100%]
max-w-[300px] rounded-xl" poster="/img/tutorials/alpha-graph2.jpg"> <source
src="/img/tutorials/alpha-graph2.webm" type="video/webm"/> <source
src="/img/tutorials/alpha-graph2.mp4" type="video/mp4"/> </video> </figure>

  * Now you can play with the _“Distance”_ and _“Scale”_ sliders.
    * _“Distance”_ slices the volume away from the camera, revealing more of the inner structure.
    * _“Scale”_ is simple zooming.
  * See if you can uncover the spiral on the last page of the scroll:

<figure> <img src="/img/tutorials/spiral.png" /> </figure>

Here is a video showing the whole process:

<figure> <video controls playsInline muted className="w-[100%] rounded-xl"
poster="/img/tutorials/imagej-word-soup2.jpg"> <source
src="/img/tutorials/imagej-word-soup2.webm" type="video/webm"/> <source
src="/img/tutorials/imagej-word-soup2.mp4" type="video/mp4"/> </video>
</figure>

That is a what we call a “word soup” — letters floating in the air. Our real
datasets aren’t nearly as pretty as this, but this should you give some idea
of what volumetric data of papyrus scrolls look like in the best case
scenario.

However, it’s still hard to read anything. “Tutorial 3: Virtual Unwrapping”
should help with this.

### Meshes

There is another way we can visualize the volume: we can convert it into
triangles. A set of triangles is called a “mesh”, and it allows us to view our
volume in tools that don’t support image stacks directly, which is sometimes
useful. For this, we need to export to a mesh from Fiji:

  * Go back to the tomographically reconstructed image stack in Fiji (or import it again using _“File > Import > Image Sequence”_).
  * Go to _“Image > Type (at the very top) > RGB Color”_ (this is required for the meshing algorithm).
  * Go to _“File > Save As > Wavefront .OBJ (near the bottom)”_
  * Now choose an intensity threshold above which we consider the object “solid”. A value of _“40”_ works well for the campfire scroll.
  * Press _“OK”_ and save the file somewhere.
  * Wait a moment until the file is saved. It should be around 400MB.

<figure className="max-w-[200px]"> <img src="/img/tutorials/mesh-export2.png"
/> </figure>

Now, download and install MeshLab.

  * Use _“File > Import”_, or drag the file into MeshLab.
    * It can take a while to import; there is a progress bar at the bottom.
  * In the right panel, click on the cylinder icon (see screenshot below), then click on _“Back-Face > Double”_.
  * Drag around to rotate the mesh.

<figure> <img src="/img/tutorials/meshlab.png" className="rounded-xl" />
</figure>

You can quite clearly see the different pieces of papyrus, and even the
container holding the scroll in place inside the scanner. However, all
information of letters is lost.

We’ll use Meshlab in later tutorials, so it’s useful to have seen it.

Here is a video of playing with Meshlab:

<figure> <video controls playsInline muted className="w-[100%] rounded-xl"
poster="/img/tutorials/meshlab2.jpg"> <source
src="/img/tutorials/meshlab2.webm" type="video/webm"/> <source
src="/img/tutorials/meshlab2.mp4" type="video/mp4"/> </video> </figure>

